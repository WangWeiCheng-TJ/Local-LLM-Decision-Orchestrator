import os
import json
import chromadb
from termcolor import cprint
import sys
from dotenv import load_dotenv

# å¼•ç”¨å·¥å…·
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json

load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")

class ProfileGeneratorAgent:
    def __init__(self, model, db_path):
        self.model = model
        self.db_path = db_path

    def _fetch_context_from_db(self):
        """
        å¾å€‹äººè³‡æ–™åº«æ’ˆå–è³‡æ–™ï¼Œä¸¦è‡ªå‹•éæ¿¾æ‰å­¸è¡“è«–æ–‡ (Noise)ã€‚
        """
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"DB not found at {self.db_path}")

        client = chromadb.PersistentClient(path=self.db_path)
        collection = client.get_collection("personal_knowledge")

        # 1. æ’ˆå–è³‡æ–™ (åŠ å¤§ limitï¼Œå› ç‚ºæˆ‘å€‘å¯èƒ½æœƒæ¿¾æ‰å¾ˆå¤šè«–æ–‡)
        # get() ä¸å¸¶ where æ¢ä»¶é è¨­æ˜¯æ’ˆæ‰€æœ‰çš„ IDï¼Œä½†ç‚ºäº†æ•ˆèƒ½æˆ‘å€‘å…ˆæŠ“å‰ 30 ç­†
        results = collection.get(limit=30)
        
        if not results['documents']:
            cprint("âŒ Database is empty! Please run ingest_personal_data.py first.", "red")
            return ""
        
        context = ""
        skipped_count = 0
        
        # å®šç¾©è¦è·³éçš„é—œéµå­— (å°å¯«)
        # å¦‚æœ ingest çš„æ™‚å€™æ¨™è¨˜äº† "Research Paper" æˆ– "ArXiv"ï¼Œé€™è£¡å°±æœƒæ“‹æ‰
        SKIP_KEYWORDS = ["paper", "arxiv", "publication", "journal", "conference", "proceeding", "thesis"]
        
        cprint(f"ğŸ” Scanning {len(results['documents'])} documents from DB...", "cyan")

        for i, doc in enumerate(results['documents']):
            # å–å¾— metadata
            meta = results['metadatas'][i] if results['metadatas'] else {}
            fname = results['ids'][i]
            
            # å–å‡ºåˆ¤æ–·ç”¨çš„æ¬„ä½
            domain = meta.get('domain', '').lower()
            tags = meta.get('tags', '').lower()
            doc_type = meta.get('doc_type', 'unknown')
            
            # === [éæ¿¾é‚è¼¯] ===
            # å¦‚æœ Domain æˆ– Tags åŒ…å« "paper", "arxiv" ç­‰å­—çœ¼ï¼Œä¸”ä¸æ˜¯æ˜ç¢ºçš„ Resumeï¼Œå°±è·³é
            is_paper = any(k in domain for k in SKIP_KEYWORDS) or \
                       any(k in tags for k in SKIP_KEYWORDS)
            
            # ç‰¹åˆ¥ä¿ç•™ï¼šå¦‚æœæª”åæˆ–æ¨™ç±¤æ˜ç¢ºèªªæ˜¯ Resume/CVï¼Œå°±ç®—å®ƒè¢«æ¨™æˆ paper ä¹Ÿè¦ç•™è‘—
            is_resume_flag = meta.get('is_resume', 'False').lower() == 'true'
            
            if is_paper and not is_resume_flag:
                cprint(f"   ğŸš« Skipping Paper: {fname} (Domain: {meta.get('domain')})", "dark_grey")
                skipped_count += 1
                continue
                
            # === [åŠ å…¥ Context] ===
            cprint(f"   ğŸ“¥ Loading: {fname} (Domain: {meta.get('domain')})", "white")
            # æ¯å€‹æª”æ¡ˆæ“·å–å‰ 5000 å­—ï¼Œé¿å… Context Window çˆ†æ‰
            context += f"\n=== FILE: {fname} (Type: {doc_type}, Domain: {meta.get('domain')}) ===\n{doc[:5000]}\n"
        
        if skipped_count > 0:
            cprint(f"   (Filtered out {skipped_count} academic/paper documents to reduce noise)", "yellow")
            
        return context

    def generate_profile(self) -> str:
        cprint("ğŸ§  Profile Generator extracting insights from Personal DB...", "cyan")
        
        context = self._fetch_context_from_db()
        if not context:
            return "Error: No relevant personal data found (Papers were filtered out)."

        # === Analysis Prompt ===
        prompt = f"""
        You are a Career Agent analyzing the user's **Personal Knowledge Base**.
        
        ### SOURCE DATA (Filtered Personal Notes & Records):
        {context}

        ### MISSION:
        Organize this information into a cohesive **Job Triage Profile**.
        Infer the user's seniority, skills, and preferences based on their actual work records.

        ### INFERENCE TASKS:
        1. **Education**: Does the user mention a PhD or Lab work?
        2. **Current Context**: Where are they based?
        3. **True Level**: Based on the *technical depth* of these notes, are they Junior, Senior, or Expert?
        4. **Tech Stack**: 
           - **Primary**: What tools appear in active, positive contexts?
           - **Anti-Stack**: What tools are absent or mentioned negatively? (Infer: If only AI/Python is present, assume Web/Legacy stacks are unwanted).
        5. **Role Fit**: What job titles match the work described here?

        ### OUTPUT JSON SCHEMA:
        {{
            "education_level": "...",
            "current_location": "...", 
            "seniority_level": "...",
            "primary_stack": ["..."],
            "anti_stack": ["..."], 
            "target_roles": ["..."],
            "avoid_roles": ["..."],
            "relocation_inference": "..."
        }}
        """

        data = safe_generate_json(self.model, prompt)
        
        # è½‰æˆ Markdown (Triage ç”¨)
        markdown_output = f"""# ğŸ›¡ï¸ Personal Triage Profile (Auto-Generated)
> **Source**: Personal Database (Papers Filtered)
> **Date**: (Auto)

## 1. ğŸ“ Professional Core
- **Education**: {data.get('education_level', 'Unknown')}
- **Inferred Level**: {data.get('seniority_level', 'Senior')}
- **Target Roles**: {", ".join(data.get('target_roles', []))}
- **Roles to Avoid**: {", ".join(data.get('avoid_roles', []))}

## 2. ğŸŒ Location & Relocation
- **Current Base**: {data.get('current_location', 'Unknown')}
- **Inferred Preference**: {data.get('relocation_inference', 'Unknown')}

## 3. ğŸ› ï¸ Tech Stack Strategy
- **ğŸš€ Primary Stack (Keep)**: 
  {", ".join(data.get('primary_stack', []))}

- **ğŸ›‘ Anti-Stack (Reject)**: 
  *Inferred from lack of presence or context in DB.*
  {", ".join(data.get('anti_stack', []))}

## 4. ğŸ§  Agent Observations
- Analyzed {len(context) // 100} units of personal context (excluding papers).
- Inferred focus: {", ".join(data.get('primary_stack', [])[:3])}.
"""
        return markdown_output

if __name__ == "__main__":
    from dotenv import load_dotenv
    import google.generativeai as genai
    
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    chroma_path = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(MODEL_NAME)
    
    agent = ProfileGeneratorAgent(model, chroma_path)
    print(agent.generate_profile())