import os
import glob
import time
import json
import chromadb
import google.generativeai as genai
from termcolor import colored, cprint
from dotenv import load_dotenv
from tqdm import tqdm
import sys

# === å¼•å…¥å·¥å…· ===
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json, extract_text_from_pdf

load_dotenv()
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")

# è·¯å¾‘è¨­å®š
PATH_ONGOING = "/app/data/history/ongoing"
PATH_REJECTED = "/app/data/history/rejected"
FORCE_UPDATE = os.getenv("FORCE_UPDATE", "False").lower() == "true"

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

# ==========================================
# ğŸ§  1. Parsers (é‡å°ä¸åŒæ–‡ä»¶é¡å‹çš„è§£æå™¨)
# ==========================================

def parse_resume_to_structured_data(text):
    """å°‡å±¥æ­·è½‰ç‚ºçµæ§‹åŒ– JSON"""
    prompt = f"""
    You are a Resume Parser. Extract structured data from this resume text.
    
    ### RESUME TEXT:
    {text}
    
    ### TARGET SCHEMA (JSON):
    {{
        "summary": "Professional summary",
        "education": [ {{ "degree": "...", "school": "...", "year": "..." }} ],
        "work_experience": [ {{ "title": "...", "company": "...", "duration": "...", "key_responsibilities": "..." }} ],
        "technical_skills": {{ "languages": [], "frameworks": [], "tools": [] }},
        "PUBLICATIONS": [ {{ "name": "...", "publisher": "..." , "year": "..."}},
        "soft_skills": {{ "Leadership": [], "Innovation": [], "Presentations": [] }},
         ]
    }}
    """
    return safe_generate_json(model, prompt)

def indexer_agent_jd(text):
    """åˆ†æ JD (Job Description)"""
    prompt = f"""
    You are analyzing a PAST JOB APPLICATION (JD).
    Snippet: {text}
    
    Extract JSON:
    {{
        "role": "Job Title",
        "company": "Company Name",
        "experience_level": "Senior/Junior/...",
        "tech_stack": ["Skill1", "Skill2"],
        "summary": "One liner summary of the job",
        "tags": ["#Tag1"]
    }}
    """
    default = {"role": "Unknown", "company": "Unknown", "experience_level": "Unknown", "tech_stack": [], "summary": "", "tags": []}
    return safe_generate_json(model, prompt, default_output=default)

def parser_cover_letter(text):
    """åˆ†æ Cover Letter"""
    prompt = f"""
    Analyze this Cover Letter.
    Snippet: {text}
    
    Extract JSON:
    {{
        "target_role": "Role applied for",
        "target_company": "Company applied to",
        "key_selling_points": ["Point 1", "Point 2"],
        "connection": "How to apply skills to this role"
    }}
    """
    default = {"target_role": "Unknown", "target_company": "Unknown", "key_selling_points": [], "connection": "Unknown"}
    return safe_generate_json(model, prompt, default_output=default)

# ==========================================
# ğŸ•µï¸ 2. Classifier (åˆ†é¡å™¨)
# ==========================================

def identify_doc_type(filename, text):
    """
    åˆ¤æ–·æ–‡ä»¶é¡å‹ï¼šJD, RESUME, COVER_LETTER
    å„ªå…ˆçœ‹æª”åï¼Œå¦‚æœæª”åçœ‹ä¸å‡ºä¾†ï¼Œçœ‹å…§å®¹å‰ 2000 å­—
    """
    fname = filename.lower()
    
    # 1. å¿«é€Ÿæª”åè¦å‰‡ (Heuristics)
    if "resume" in fname or "cv" in fname:
        return "RESUME"
    if "cover" in fname and "letter" in fname:
        return "COVER_LETTER"
    if "jd" in fname or "job" in fname or "description" in fname:
        return "JD"
        
    # 2. å¦‚æœæª”åæ¨¡ç³Š (ä¾‹å¦‚ "Google_2023.pdf")ï¼Œç”¨ LLM åˆ¤æ–·
    prompt = f"""
    Classify this document based on the snippet.
    Filename: {filename}
    Snippet: {text[:1000]}
    
    Options: ["RESUME", "COVER_LETTER", "JD", "OTHER"]
    Return JSON: {{ "doc_type": "..." }}
    """
    res = safe_generate_json(model, prompt, default_output={"doc_type": "JD"}) # é è¨­ç•¶ä½œ JD
    return res.get("doc_type", "JD")

# ==========================================
# ğŸš€ 3. Processor (ä¸»æµç¨‹)
# ==========================================

def extract_text_smart(filepath):
    return extract_text_from_pdf(filepath, model_name=MODEL_NAME)

def process_folder(base_path, status_label, collection):
    search_path = os.path.join(base_path, "**", "*.pdf")
    files = glob.glob(search_path, recursive=True)
    
    if not files: return 0

    count = 0
    skipped_count = 0
    
    pbar = tqdm(files, desc=f"Processing {status_label}", unit="file")

    for filepath in pbar:
        filename = os.path.basename(filepath)
        folder_name = os.path.basename(os.path.dirname(filepath))
        pbar.set_postfix(file=filename[:15])

        # 1. è¨ˆç®— ID
        safe_status = status_label.replace("/", "_").replace(" ", "_")
        doc_id = f"history_{safe_status}_{folder_name}_{filename}"
        
        # 2. Check Existing
        if not FORCE_UPDATE:
            existing = collection.get(ids=[doc_id])
            if existing and existing['ids']:
                skipped_count += 1
                continue 

        # 3. Extract Text
        text, used_ocr = extract_text_smart(filepath)
        if not text or len(text) < 50:
            tqdm.write(colored(f"   âš ï¸ [Skip] Empty content: {filename}", "yellow"))
            continue

        # 4. Classify Document
        pbar.set_description(f"ğŸ” Classifying: {filename[:10]}...")
        doc_type = identify_doc_type(filename, text)
        
        # 5. Route & Analyze
        pbar.set_description(f"ğŸ¤– Analyzing [{doc_type}]: {filename[:10]}...")
        
        analysis_result = {}
        role_tag = "Unknown"
        
        if doc_type == "RESUME":
            analysis_result = parse_resume_to_structured_data(text)
            role_tag = "Candidate" # Resume ä¸ä¸€å®šæœ‰ç‰¹å®š Role
            
        elif doc_type == "COVER_LETTER":
            analysis_result = parser_cover_letter(text)
            role_tag = analysis_result.get("target_role", "Unknown")
            
        else: # Default to JD
            analysis_result = indexer_agent_jd(text)
            role_tag = analysis_result.get("role", "Unknown")

        # 6. Prepare Metadata
        # æ³¨æ„ï¼šChromaDB metadata åªèƒ½å­˜ string/int/float/boolï¼Œä¸èƒ½å­˜ dict
        # æ‰€ä»¥è¦æŠŠçµæ§‹åŒ–è³‡æ–™ json.dumps è½‰æˆå­—ä¸²
        
        storage_meta = {
            "source": "history",
            "folder": folder_name,
            "filename": filename,
            "status": status_label,
            "doc_type": doc_type, # é—œéµæ¬„ä½ï¼
            "role": role_tag[:50], # é¿å…å¤ªé•·
            "summary": str(analysis_result.get("summary", ""))[:200],
            "analysis_json": json.dumps(analysis_result, ensure_ascii=False) # <--- æœ€ç²¾è¯çš„çµæ§‹åŒ–è³‡æ–™å­˜åœ¨é€™
        }

        # 7. Upsert
        collection.upsert(
            documents=[text],
            metadatas=[storage_meta],
            ids=[doc_id]
        )
        
        # Log Result
        type_color = "cyan" if doc_type == "JD" else "magenta" if doc_type == "RESUME" else "yellow"
        type_icon = "ğŸ“„" if doc_type == "JD" else "ğŸ“" if doc_type == "RESUME" else "âœ‰ï¸"
        
        msg = colored(f"   âœ… {type_icon} [{doc_type}] Indexed: {folder_name}/{filename[:20]}", "green")
        tqdm.write(msg)
        
        count += 1
        pbar.set_description(f"Processing {status_label}")
        if used_ocr: time.sleep(1)

    if skipped_count > 0:
        tqdm.write(colored(f"   (Skipped {skipped_count} existing files)", "light_grey"))
        
    return count

def ingest_history_jds():
    cprint("\nğŸ“œ [Level 0] Building History Index (Smart Mode)...", "cyan", attrs=['bold'])
    
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    # æˆ‘å€‘å¯ä»¥ç¹¼çºŒç”¨åŒä¸€å€‹ collectionï¼Œé  metadata['doc_type'] å€åˆ†å³å¯
    collection = client.get_or_create_collection(name="past_applications_jds")
    
    total_new = 0
    
    if os.path.exists(PATH_ONGOING):
        total_new += process_folder(PATH_ONGOING, "Ongoing", collection)
    
    print("-" * 40)
    
    if os.path.exists(PATH_REJECTED):
        total_new += process_folder(PATH_REJECTED, "Rejected", collection)

    cprint(f"\nâœ… All Done! Added {total_new} new records.", "magenta", attrs=['bold'])

if __name__ == "__main__":
    ingest_history_jds()