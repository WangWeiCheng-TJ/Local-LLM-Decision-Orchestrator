import os
import glob
import chromadb
import google.generativeai as genai
from termcolor import cprint
from dotenv import load_dotenv
import json

# å¼•å…¥é˜²å‘†å·¥å…· (è«‹ç¢ºä¿ src/utils/llm_utils.py å­˜åœ¨)
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from src.utils import safe_generate_json
from src.utils import extract_text_from_pdf

load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash")
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
RAW_DATA_PATH = "/app/data/raw" # é€™è£¡æ”¾ä½ æ‰€æœ‰çš„å€‹äººè³‡æ–™ (PDF/MD/TXT)

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

def extract_text(file_path):
    """
    æ™ºæ…§è®€å–ï¼šå…ˆå˜—è©¦ä¸€èˆ¬è®€å–ï¼Œè®€ä¸åˆ°å°±åˆ‡æ› OCRã€‚
    """
    ext = os.path.splitext(file_path)[1].lower()
    filename = os.path.basename(file_path)

    try:
        # === è™•ç† PDF ===
        if ext == ".pdf":
            # ä½¿ç”¨ utils ä¸­çš„ extract_text_from_pdf (åŸºæ–¼ utils.py:12)
            text, used_ocr = extract_text_from_pdf(file_path, model_name=MODEL_NAME)
            # [ä¿®æ­£é» 1] å›å‚³é€šç”¨çš„ "pdf_document"ï¼Œä¸è¦åœ¨é€™è£¡å®šæ­»å®ƒæ˜¯ resume
            return text, "pdf_document"

        # === è™•ç†ç­†è¨˜ (MD/TXT) ===
        elif ext in [".md", ".txt"]:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read(), "personal_note"

        # === [NEW] è™•ç† JSON (user_profile.json) ===
        elif ext == ".json":
            with open(file_path, "r", encoding="utf-8") as f:
                json_content = json.load(f)
                
                # å¦‚æœæ˜¯ user_profile.jsonï¼Œæ¨™è¨˜ç‚ºç‰¹æ®Šé¡å‹ï¼Œä¸è¦éåº¦ summarize
                if filename == "user_profile.json":
                    text = json.dumps(json_content, indent=2, ensure_ascii=False)
                    return text, "user_profile"  # ç‰¹æ®Š doc_type
                else:
                    text = json.dumps(json_content, indent=2, ensure_ascii=False)
                    return text, "structured_data"

        else:
            return None, None

    except Exception as e:
        cprint(f"âŒ è®€å–æª”æ¡ˆå¤±æ•— {file_path}: {e}", "red")
        return None, None

def indexer_agent_process(filename, text, doc_type):
    # å¦‚æœæ˜¯ user_profileï¼Œç›´æ¥è·³é LLMï¼Œç”¨åŸå§‹ metadata
    if doc_type == "user_profile":
        return {
            "summary": "User Profile (Pre-computed cheat sheet)",
            "domain": "Career Profile",
            "tags": ["#UserProfile", "#Skills", "#Education"],
            "is_resume": False
        }
    else:
        prompt = f"""
        You are my Personal Data Archivist.
        I am ingesting a document into my personal knowledge base.
        
        Filename: {filename}
        Type: {doc_type}
        Content Snippet: {text}
        
        ### TASK
        1. Identify the **Topic/Domain** (e.g., "Resume V1", "Project Alpha Notes", "Research Idea").
        2. Extract **Keywords/Skills** mentioned.
        3. Summarize the content in one sentence.
        
        ### OUTPUT JSON
        {{
            "summary": "Brief summary of this file.",
            "domain": "Computer Vision / System Design / Career Profile",
            "tags": ["#Tag1", "#Tag2"],
            "is_resume": true/false
        }}
        """
        
        default_res = {
            "summary": "Processing Failed",
            "domain": "Unknown",
            "tags": [],
            "is_resume": False
        }

    return safe_generate_json(model, prompt, retries=3, default_output=default_res)

def ingest_personal_data():
    cprint(f"ğŸš€ [Level 0] é–‹å§‹å»ºç½®å€‹äººçŸ¥è­˜åº« (Ingesting Personal Data)...", "cyan", attrs=['bold'])
    
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    # æˆ‘å€‘æŠŠ Collection åå­—æ”¹å¾—æ›´é€šç”¨ä¸€é»ï¼Œå« "personal_knowledge"
    collection = client.get_or_create_collection(name="personal_knowledge")
    
    files = glob.glob(os.path.join(RAW_DATA_PATH, "*"))
    
    count = 0
    for file_path in files:
        filename = os.path.basename(file_path)
        
        # 1. è®€å–
        content, doc_type = extract_text(file_path)
        if not content: continue
        
        cprint(f"\nğŸ“„ åˆ†ææª”æ¡ˆ: {filename} ({doc_type})", "white")

        # 2. AI ç†è§£ & æ¨™è¨˜
        cprint("   ğŸ¤– Indexer Agent Analyzing...", "blue")
        metadata = indexer_agent_process(filename, content, doc_type)

        cprint(f"   ğŸ·ï¸ Domain: {metadata.get('domain')}", "green")
        cprint(f"   ğŸ“ Summary: {metadata.get('summary')}", "green")

        # 3. æ ¼å¼åŒ– Metadata
        storage_meta = {
            "filename": filename,
            "doc_type": doc_type,
            "domain": metadata.get("domain", "Unknown"),
            "tags": ", ".join(metadata.get("tags", [])),
            "is_resume": str(metadata.get("is_resume", False)), # Chroma ä¸å­˜ boolï¼Œè½‰å­—ä¸²
            "summary": metadata.get("summary", "")
        }

        # 4. å­˜å…¥ (æ•´ä»½å­˜å…¥ï¼Œä¸åˆ‡å¡Šï¼Œä¿æŒå®Œæ•´èªæ„)
        try:
            collection.upsert(
                documents=[content],
                metadatas=[storage_meta],
                ids=[filename]
            )
            cprint("   âœ… Saved to Knowledge Base", "magenta")
            count += 1
        except Exception as e:
            cprint(f"âŒ DB Error: {e}", "red")

    cprint(f"\nğŸ‰ å»ºç½®å®Œæˆï¼ä½ çš„æ•¸ä½åˆ†èº«ç¾åœ¨æ“æœ‰ {count} ä»½è¨˜æ†¶ã€‚", "cyan", attrs=['bold'])

if __name__ == "__main__":
    ingest_personal_data()