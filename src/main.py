import os
import re
import glob
import time
import json
import csv
import google.generativeai as genai
import chromadb
from termcolor import cprint
from dotenv import load_dotenv
from pypdf import PdfReader
from pathlib import Path
from utils import gemini_ocr
from utils import identify_application_packet

# --- é…ç½®å€ ---
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-pro")
CHROMA_PATH = os.getenv("CHROMA_DB_PATH", "/app/data/chroma_db")
INPUT_DIR = "/app/data/jds"
OUTPUT_DIR = "/app/data/reports"
RAW_DIR = "/app/data/raw"  # [æ–°å¢] ç”¨ä¾†è®€ AboutMe.md

# åˆå§‹åŒ–
genai.configure(api_key=API_KEY)
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

class PrivacyShield:
    def __init__(self):
        self.patterns = {
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}': '[EMAIL_REDACTED]',
            r'\+?[0-9\s\-\(\)]{8,}': '[PHONE_REDACTED]',
        }
    def sanitize(self, text):
        for pattern, replacement in self.patterns.items():
            text = re.sub(pattern, replacement, text)
        return text

class AgentBrain:
    def __init__(self):
        self.model = genai.GenerativeModel(MODEL_NAME)
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
        self.memory = self.chroma_client.get_or_create_collection(name="job_experiences")
        self.shield = PrivacyShield()

        # [æ–°å¢] å•Ÿå‹•æ™‚è¼‰å…¥ User Values
        self.user_values = self.load_user_profile()

    def load_user_profile(self):
        """ [æ–°åŠŸèƒ½] å¾ raw/AboutMe.md è®€å–åƒ¹å€¼è§€ï¼Œè€Œä¸æ˜¯å¯«æ­»åœ¨ Code è£¡ """
        profile_path = os.path.join(RAW_DIR, "AboutMe.md")
        default_values = "- Goal: Find a job.\n- Visa: Need sponsorship."
        
        if os.path.exists(profile_path):
            try:
                with open(profile_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    cprint("   ğŸ‘¤ å·²è¼‰å…¥å€‹äººåƒ¹å€¼è§€ (AboutMe.md)", "cyan")
                    return content
            except Exception as e:
                cprint(f"   âš ï¸ è®€å– AboutMe.md å¤±æ•—: {e}", "red")
        else:
            cprint("   âš ï¸ æ‰¾ä¸åˆ° AboutMe.mdï¼Œä½¿ç”¨é è¨­å€¼", "yellow")
        
        return default_values

    # def ocr_image_pdf(self, filepath):
    #     cprint(f"   ğŸ‘ï¸ å•Ÿå‹• Gemini Vision é€²è¡Œé›²ç«¯ OCR...", "magenta")
    #     try:
    #         sample_file = genai.upload_file(path=filepath, display_name="JD File")
    #         while sample_file.state.name == "PROCESSING":
    #             time.sleep(1)
    #             sample_file = genai.get_file(sample_file.name)
            
    #         prompt = "Extract all text from this document accurately."
    #         response = self.model.generate_content([sample_file, prompt])
    #         return response.text
    #     except Exception as e:
    #         cprint(f"   âŒ Cloud OCR å¤±æ•—: {e}", "red")
    #         return None

    def generate_search_query(self, jd_text):
        """ ç”¨ AI æå–é—œéµå­— """
        try:
            prompt = f"""
            Find the minimal and preferred requirements from this Job Description.
            Return ONLY a comma-separated string.
            JD: {jd_text[:3000]}
            """
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except:
            return jd_text[:500]

    def recall_past_lessons(self, jd_text):
        """ 
        [ä¿®æ­£] å…ˆç”¨ generate_search_query æå–é—œéµå­—ï¼Œå†å»æœæ­·å² JD 
        """
        # 1. å–å¾—æ­·å²è³‡æ–™åº« (å¦‚æœ ingest_history.py æ²’è·‘éï¼Œé€™è£¡æœƒæ˜¯ç©ºçš„)
        history_collection = self.chroma_client.get_or_create_collection(name="past_applications_jds")
        if history_collection.count() == 0:
            return "No historical data indexed yet."

        # 2. [é—œéµä¿®æ­£] ä½¿ç”¨æå–å‡ºçš„ Keyword é€²è¡Œæœå°‹ï¼Œè€ŒéåŸå§‹ JD å…¨æ–‡
        search_query = self.generate_search_query(jd_text)
        # cprint(f"   ğŸ•°ï¸ æ­·å²æª¢ç´¢é—œéµå­—: {search_query}", "cyan")

        results = history_collection.query(
            query_texts=[search_query], 
            n_results=1
        )
        
        lesson_context = "No similar past application found."
        
        if results['documents'] and results['documents'][0]:
            # é€™è£¡ç°¡å–®åˆ¤æ–·è·é›¢ï¼Œå¯¦éš›å¯èª¿
            meta = results['metadatas'][0][0]
            folder_path = meta['folder_path']
            company_role = meta['company_role']
            
            # å˜—è©¦è®€å– outcome
            outcome_text = "Unknown"
            outcome_files = glob.glob(os.path.join(folder_path, "*outcome*")) + glob.glob(os.path.join(folder_path, "*reject*"))
            if outcome_files:
                try:
                    with open(outcome_files[0], 'r', encoding='utf-8', errors='ignore') as f:
                        outcome_text = f.read()[:500] # åªè®€å‰500å­—é¿å…å¤ªé•·
                        # æœªä¾†æ‡‰è©²è¦æ”¹æˆ smart_extract_text
                except: pass

            lesson_context = f"""
            *** HISTORY RECALL ALERT ***
            This new job matches keywords with a past application: '{company_role}'.
            
            Path: {folder_path}
            Past Outcome: {outcome_text}
            
            STRATEGIC INSTRUCTION:
            - If outcome was POSITIVE: Check the resume in that folder for reusable phrasing.
            - If outcome was NEGATIVE: Analyze the outcome text to avoid repeating mistakes.
            """
            
        return lesson_context

    def retrieve_context(self, jd_text, n_results=3):
        # ä½¿ç”¨é—œéµå­—æœå°‹å€‹äººèƒŒæ™¯
        query_text = self.generate_search_query(jd_text)
        results = self.memory.query(query_texts=[query_text], n_results=n_results)
        
        context_str = ""
        sources_list = []
        if results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                source = meta.get('source', 'Unknown')
                sources_list.append(source)
                context_str += f"\n[Evidence {i+1} from {source}]:\n{doc}\n"
        return context_str, list(set(sources_list))
    
    # åœ¨ AgentBrain class å…§æ–°å¢
    def recall_past_lessons(self, current_jd_text):
        """
        æœå°‹æ­·å²è³‡æ–™åº«ï¼Œçœ‹æœ‰æ²’æœ‰é¡ä¼¼çš„è·ç¼ºï¼Œä¸¦æå–ç•¶æ™‚çš„ç­–ç•¥èˆ‡çµæœã€‚
        """
        # 1. é€£æ¥æ­·å² JD è³‡æ–™åº«
        history_collection = self.chroma_client.get_collection(name="past_applications_jds")
        
        # 2. æœå°‹æœ€åƒçš„ 1 ç­†
        results = history_collection.query(
            query_texts=[current_jd_text[:1000]], # ç”¨æ–° JD çš„å‰ 1000 å­—å»æœ
            n_results=1
        )
        
        lesson_context = "No similar past application found."
        
        if results['documents'] and results['documents'][0]:
            # æ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹äº†ï¼
            similarity_dist = results['distances'][0][0] # è·é›¢è¶Šå°è¶Šåƒ
            meta = results['metadatas'][0][0]
            folder_path = meta['folder_path']
            company_role = meta['company_role']
            
            # è¨­å®šä¸€å€‹ç›¸ä¼¼åº¦é–€æª» (è¦–è·é›¢ç®—æ³•è€Œå®šï¼Œå‡è¨­æ˜¯ L2 distance)
            # é€™è£¡å…ˆå‡è¨­å¦‚æœæœ‰æ‰¾åˆ°å°±å›å‚³ï¼Œè®“ LLM è‡ªå·±åˆ¤æ–·åƒä¸åƒ
            
            # 3. å»é‚£å€‹è³‡æ–™å¤¾æŒ–å‡ºç•¶æ™‚çš„ Resume å’Œ Outcome (å¦‚æœæœ‰)
            
            packet = identify_application_packet(folder_path)
            
            outcome_text = "Unknown/Pending"
            if packet['outcome']:
                with open(packet['outcome'], 'r', encoding='utf-8', errors='ignore') as f:
                    outcome_text = f.read()
            
            resume_path = packet['resume'] if packet['resume'] else "Unknown"

            lesson_context = f"""
            *** HISTORY RECALL ALERT ***
            This new job is highly similar to a past application: '{company_role}'.
            
            Path: {folder_path}
            Past Outcome: {outcome_text}
            
            STRATEGIC ADVICE REQUEST:
            - If the past outcome was POSITIVE (Interview), checking the resume at '{resume_path}' might reveal reusable keywords or phrasing.
            - If the past outcome was NEGATIVE (Reject), analyze the outcome text above to avoid the same mistake.
            """
            
        return lesson_context

    def think(self, jd_text, filename):
        safe_jd = self.shield.sanitize(jd_text)
        
        # 1. RAG: æ‰¾æˆ‘çš„ç›¸é—œç¶“é©—
        retrieved_knowledge, sources = self.retrieve_context(safe_jd)
        source_msg = ', '.join(sources) if sources else "None"

        # 2. History RAG: æ‰¾é¡ä¼¼çš„æˆ°å½¹
        history_insight = self.recall_past_lessons(jd_text)

        # 3. [ä¿®æ­£] Prompt å¼·åŒ–ï¼šè®€å–å¤–éƒ¨ AboutMeï¼Œä¸¦æ•´åˆ Agent 3/4
        prompt = f"""
        You are a specialized Career Agent. Target Job File: {filename}
        
        USER CONTEXT (My background from RAG):
        {retrieved_knowledge}
        
        USER VALUES (From AboutMe.md):
        {self.user_values}

        TARGET JOB DESCRIPTION (JD):
        {safe_jd}

        === ğŸ›ï¸ HISTORICAL BATTLE DATA (Relevant Past Application) ===
        {history_insight}
        ===========================================================
        
        MISSION:
        1. **Extract & Compare**: Identify top 5 Hard Requirements from the JD and check if "USER CONTEXT" covers them.
        2. **Persona Analysis**: Analyze using the 3-Agent Persona.
        3. **Scoring**: Output JSON scoring.

        ### ğŸ” GAP ANALYSIS (Requirements vs. My Skills)
        - List the Top 5 Hard Requirements (Skills/Experience).
        - For each, verify if "USER CONTEXT" provides evidence.
        - **Verdict**: [MATCH / GAP / PARTIAL]

        ### ğŸ›¡ï¸ AGENT 1: BLIND-SPOT RADAR
        (Hidden costs, tax traps, tech debt scanning)

        ### ğŸ’€ AGENT 2: DEVIL'S ADVOCATE
        (Pre-mortem: Why will I get rejected? Why will I hate this job?)

        ### â™Ÿï¸ AGENT 3: THE STRATEGIST
        - Focus on bridging the Gaps identified above.
        - **HISTORY CHECK**: specifically look at "HISTORICAL BATTLE DATA".
          - If we applied to a similar job before (e.g., Company X), tell me explicitly: "Reuse the strategy/intro from [Company X]."
          - Or warn me: "Last time with [Company X], you failed because of [Reason]. Fix it this time."

        ---
        ### ğŸ“Š SCORING (JSON Format)
        Provide valid JSON inside ```json``` tags.
        Keys: "company_name", "role_name", "match_score" (0-100), "risk_level" (Low/Medium/High), "salary_potential", "visa_friendly", "one_line_summary".
        """

        response = self.model.generate_content(prompt)
        return response.text, source_msg

def smart_extract_text(filepath, agent):
    path = Path(filepath)
    text = ""
    
    # --- å¿«å–æ©Ÿåˆ¶ (Caching Strategy) ---
    # å¦‚æœæ˜¯ PDFï¼Œå…ˆæª¢æŸ¥æ—é‚Šæœ‰æ²’æœ‰åŒåçš„ .txt
    cached_txt_path = path.with_suffix('.txt')
    
    if path.suffix.lower() == '.pdf' and cached_txt_path.exists():
        cprint(f"   âš¡ ç™¼ç¾æœ¬åœ°ç·©å­˜ (Cached Text): {cached_txt_path.name}", "cyan")
        try:
            with open(cached_txt_path, "r", encoding="utf-8") as f:
                content = f.read()
            if len(content) > 50:
                return content
        except Exception:
            cprint("   âš ï¸ ç·©å­˜è®€å–å¤±æ•—ï¼Œé‡æ–°é€²è¡Œæå–...", "yellow")

    # --- æ²’ç·©å­˜ï¼Œé–‹å§‹æå– ---
    try:
        if path.suffix.lower() == '.pdf':
            reader = PdfReader(filepath)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted: text += extracted + "\n"
        else:
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read()
    except Exception:
        pass

    # --- åˆ¤æ–·æ˜¯å¦éœ€è¦ OCR ---
    if len(text.strip()) < 50 and path.suffix.lower() == '.pdf':
        cprint(f"   âš ï¸ æœ¬åœ°æå–å¤±æ•—ï¼Œåˆ‡æ›è‡³ Cloud OCR...", "yellow")
        text = gemini_ocr(filepath, model_name=MODEL_NAME)
        
        # --- OCR æˆåŠŸå¾Œï¼Œå¯«å…¥ç·©å­˜ ---
        if text and len(text) > 50:
            try:
                with open(cached_txt_path, "w", encoding="utf-8") as f:
                    f.write(text)
                cprint(f"   ğŸ’¾ OCR çµæœå·²ä¿å­˜è‡³: {cached_txt_path.name}", "blue")
            except Exception as e:
                cprint(f"   âŒ ç·©å­˜å¯«å…¥å¤±æ•—: {e}", "red")
    
    return text

def extract_json_score(text):
    try:
        match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if match: return json.loads(match.group(1))
        match = re.search(r"(\{.*\"match_score\".*\})", text, re.DOTALL)
        if match: return json.loads(match.group(1))
    except Exception:
        pass
    return None

def batch_process():
    cprint(f"ğŸš€ å•Ÿå‹•æˆ°ç•¥åˆ†ææ¨¡å¼ (With Local Caching)", "cyan")
    
    files = sorted(glob.glob(os.path.join(INPUT_DIR, "*")))
    # åªæŠ“ .pdf å’Œ .txt (.md)
    files = [f for f in files if f.lower().endswith(('.pdf', '.txt', '.md'))]

    if not files:
        cprint("âš ï¸ data/jds/ ç›®éŒ„ç‚ºç©º", "red")
        return

    agent = AgentBrain()
    leaderboard_data = []
    
    # å»ºç«‹ä¸€å€‹å·²è™•ç†çš„é›†åˆï¼Œé¿å…é‡è¤‡è™•ç† (ä¾‹å¦‚åŒæ™‚æœ‰ JD.pdf å’Œ JD.txt)
    processed_stems = set()

    for idx, filepath in enumerate(files):
        filename = os.path.basename(filepath)
        file_stem = os.path.splitext(filename)[0] # æª”åä¸å«å‰¯æª”å
        path_obj = Path(filepath)

        # é‚è¼¯å„ªåŒ–ï¼šå¦‚æœé€™å€‹æª”åçš„ PDF å·²ç¶“è™•ç†éï¼Œæˆ–æ˜¯ç¾åœ¨é‡åˆ° TXT ä½†æ—é‚Šæœ‰ PDFï¼Œå°±è·³é TXT
        # (å„ªå…ˆè™•ç† PDFï¼Œå› ç‚º PDF è™•ç†æµç¨‹æœƒè‡ªå‹•è®€/å¯« TXT)
        if path_obj.suffix.lower() == '.txt':
             pdf_version = path_obj.with_suffix('.pdf')
             if pdf_version.exists():
                 # è®“è¿´åœˆè·‘åˆ° PDF é‚£ä¸€æ¬¡å†è™•ç†ï¼Œé€™è£¡å…ˆè·³é
                 continue
        
        cprint(f"[{idx+1}/{len(files)}] åˆ†æ: {filename} ...", "yellow")

        content = smart_extract_text(filepath, agent)
        if not content or len(content) < 50:
            cprint(f"   âŒ è·³é (ç„¡å…§å®¹)", "red")
            continue

        try:
            # 1. AI æ€è€ƒ
            analysis_text, used_sources = agent.think(content, filename)
            
            # 2. æå–åˆ†æ•¸
            score_data = extract_json_score(analysis_text)
            
            if score_data:
                score_data['filename'] = filename
                leaderboard_data.append(score_data)
                score = score_data.get('match_score', 0)
                risk = score_data.get('risk_level', 'Unknown')
                cprint(f"   âœ… å®Œæˆ | åˆ†æ•¸: {score} | é¢¨éšª: {risk}", "green")
            else:
                cprint(f"   âš ï¸ å®Œæˆä½†ç„¡æ³•æå–åˆ†æ•¸", "yellow")

            # 3. å­˜å ±å‘Š
            report_filename = f"Analysis_{file_stem}.md"
            with open(os.path.join(OUTPUT_DIR, report_filename), "w", encoding="utf-8") as f:
                f.write(f"# Job Analysis: {filename}\n")
                f.write(f"**Sources:** {used_sources}\n\n")
                f.write(analysis_text)
            
        except Exception as e:
            cprint(f"   âŒ Error: {e}", "red")

    # --- ç”Ÿæˆ Leaderboard CSV ---
    if leaderboard_data:
        cprint("\nğŸ“Š æ­£åœ¨ç”Ÿæˆæˆ°ç•¥æ’è¡Œæ¦œ...", "cyan")
        csv_path = os.path.join(OUTPUT_DIR, "Strategic_Leaderboard.csv")
        leaderboard_data.sort(key=lambda x: x.get('match_score', 0), reverse=True)
        keys = ["match_score", "company_name", "role_name", "risk_level", "salary_potential", "visa_friendly", "one_line_summary", "filename"]
        
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            for row in leaderboard_data:
                filtered_row = {k: row.get(k, "N/A") for k in keys}
                writer.writerow(filtered_row)     
        cprint(f"ğŸ† æ’è¡Œæ¦œå·²å»ºç«‹: {csv_path}", "green")

if __name__ == "__main__":
    batch_process()